{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12de2cb9",
   "metadata": {},
   "source": [
    "INITIALIZE & TRAIN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d6c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[skrl:INFO] Environment wrapper: Petting Zoo\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1530/50000 [02:00<52:29, 15.39it/s]  "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from floris import FlorisModel\n",
    "from pettingzoo import ParallelEnv\n",
    "\n",
    "# --- 1. MULTI-AGENT FLORIS ENVIRONMENT ---\n",
    "class FlorisMultiAgentEnv(ParallelEnv):\n",
    "    def __init__(self, config_path):\n",
    "        super().__init__()\n",
    "        # 1. Initialize the physics model\n",
    "        self.fmodel = FlorisModel(config_path)\n",
    "        D = 126.0\n",
    "        self.x_layout = [0, 0, 6 * D, 6 * D]\n",
    "        self.y_layout = [0, 3 * D, 0, 3 * D]\n",
    "        self.fmodel.set(layout_x=self.x_layout, layout_y=self.y_layout)\n",
    "        \n",
    "        # 2. DEFINE AGENTS FIRST (The Fix)\n",
    "        self.possible_agents = [f\"turbine_{i}\" for i in range(len(self.x_layout))]\n",
    "        self.agents = self.possible_agents[:]\n",
    "        \n",
    "        # 3. Define Spaces\n",
    "        obs_low = np.array([260.0, 5.0, 0.03], dtype=np.float32)\n",
    "        obs_high = np.array([290.0, 15.0, 0.25], dtype=np.float32)\n",
    "        obs_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "        \n",
    "        # Local view for Actor\n",
    "        self.observation_spaces = {a: obs_space for a in self.possible_agents}\n",
    "        \n",
    "        # Global view for Critic (Concatenated observations of all 4 turbines)\n",
    "        self.state_space = spaces.Box(\n",
    "            low=np.tile(obs_low, len(self.possible_agents)),\n",
    "            high=np.tile(obs_high, len(self.possible_agents)),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.shared_observation_spaces = {a: self.state_space for a in self.possible_agents}\n",
    "        \n",
    "        self.action_spaces = {a: spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32) \n",
    "                             for a in self.possible_agents}\n",
    "        \n",
    "        self.max_steps = 100\n",
    "        self.current_step = 0\n",
    "        self.wind_state = np.array([275.0, 10.0, 0.06], dtype=np.float32)\n",
    "    \n",
    "    def state(self):\n",
    "        # [3 params * 4 turbines]\n",
    "        return np.tile(self.wind_state, len(self.possible_agents))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.wind_state = np.array([275.0, 10.0, 0.06], dtype=np.float32)\n",
    "\n",
    "        observations = {a: self.wind_state for a in self.possible_agents}\n",
    "\n",
    "        infos = {\n",
    "            a: {\"state\": self.state()}\n",
    "            for a in self.possible_agents\n",
    "        }\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.current_step += 1\n",
    "        # Random wind drift\n",
    "        self.wind_state[0] += np.random.normal(0, 0.2)\n",
    "        self.wind_state[1] += np.random.normal(0, 0.05)\n",
    "        self.wind_state = np.clip(self.wind_state, [260, 5, 0.03], [290, 15, 0.25])\n",
    "        \n",
    "        # Apply yaws to FLORIS\n",
    "        yaws = np.array([actions[a][0] for a in self.possible_agents]) * 25.0\n",
    "        self.fmodel.set(wind_directions=[self.wind_state[0]], wind_speeds=[self.wind_state[1]], \n",
    "                        turbulence_intensities=[self.wind_state[2]], yaw_angles=np.array([yaws]))\n",
    "        self.fmodel.run()\n",
    "        \n",
    "        # Global Reward: Sum of all turbine power (encourages coordination)\n",
    "        reward = np.sum(self.fmodel.get_turbine_powers()) / 1e6\n",
    "        rewards = {a: reward for a in self.possible_agents}\n",
    "        \n",
    "        terminated = self.current_step >= self.max_steps\n",
    "        terminations = {a: terminated for a in self.possible_agents}\n",
    "        truncations = {a: False for a in self.possible_agents}\n",
    "        \n",
    "        observations = {a: self.wind_state for a in self.possible_agents}\n",
    "\n",
    "        # ✅ THIS IS THE CRITICAL FIX\n",
    "        infos = {\n",
    "            a: {\"state\": self.state()}\n",
    "            for a in self.possible_agents\n",
    "        }\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "# --- 2. MODEL DEFINITIONS ---\n",
    "from skrl.models.torch import DeterministicMixin, GaussianMixin, Model\n",
    "\n",
    "class Actor(GaussianMixin, Model):\n",
    "    def __init__(self, observation_space, action_space, device, **kwargs):\n",
    "        Model.__init__(self, observation_space, action_space, device, **kwargs)\n",
    "        GaussianMixin.__init__(self, reduction=\"sum\")\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.num_observations, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.num_actions)\n",
    "        )\n",
    "        self.log_std_parameter = nn.Parameter(torch.zeros(self.num_actions))\n",
    "\n",
    "    def compute(self, inputs, role):\n",
    "        return self.net(inputs[\"states\"]), self.log_std_parameter, {}\n",
    "\n",
    "class Critic(DeterministicMixin, Model):\n",
    "    def __init__(self, observation_space, action_space, device, **kwargs):\n",
    "        Model.__init__(self, observation_space, action_space, device, **kwargs)\n",
    "        DeterministicMixin.__init__(self)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.num_observations, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def compute(self, inputs, role):\n",
    "        return self.net(inputs[\"states\"]), {}\n",
    "\n",
    "# --- 3. INITIALIZATION AND TRAINING ---\n",
    "from skrl.envs.wrappers.torch import wrap_env\n",
    "from skrl.memories.torch import RandomMemory\n",
    "from skrl.multi_agents.torch.mappo import MAPPO, MAPPO_DEFAULT_CONFIG\n",
    "from skrl.trainers.torch import SequentialTrainer\n",
    "\n",
    "# Env Setup\n",
    "raw_env = FlorisMultiAgentEnv(\"data_generation/farm_types/gch.yaml\")\n",
    "env = wrap_env(raw_env, wrapper=\"pettingzoo\")\n",
    "\n",
    "# memory object \n",
    "memory = RandomMemory(memory_size=2000, num_envs=env.num_envs, device=env.device)\n",
    "\n",
    "# wrapped in a dictionary for MAPPO\n",
    "memories = {agent_name: memory for agent_name in env.possible_agents}\n",
    "\n",
    "# Model Sharing (Corrected spaces)\n",
    "shared_policy = Actor(env.observation_spaces[\"turbine_0\"], env.action_spaces[\"turbine_0\"], env.device)\n",
    "\n",
    "# Ensure the Critic uses the SHARED space (the 12-element one)\n",
    "shared_value = Critic(env.shared_observation_spaces[\"turbine_0\"], env.action_spaces[\"turbine_0\"], env.device)\n",
    "\n",
    "models = {a: {\"policy\": shared_policy, \"value\": shared_value} for a in env.possible_agents}\n",
    "\n",
    "# MAPPO Config Fix: Add 'state_shape' if using custom memory\n",
    "cfg_agent = MAPPO_DEFAULT_CONFIG.copy()\n",
    "cfg_agent[\"random_timesteps\"] = 0 # Start learning immediately\n",
    "cfg_agent[\"learning_rate\"] = 5e-4\n",
    "cfg_agent[\"state_preprocessor\"] = None # Optional: helps with stability \n",
    "\n",
    "# 3. Pass the DICTIONARY to the agent\n",
    "agent = MAPPO(\n",
    "    possible_agents=env.possible_agents, \n",
    "    models=models, \n",
    "    memories=memories,\n",
    "    cfg=cfg_agent, \n",
    "    observation_spaces=env.observation_spaces, \n",
    "    action_spaces=env.action_spaces, \n",
    "    device=env.device,\n",
    "    shared_observation_spaces=env.shared_observation_spaces\n",
    ")\n",
    "\n",
    "# TRAIN\n",
    "trainer = SequentialTrainer(\n",
    "    env=env, \n",
    "    agents=agent, \n",
    "    cfg={\"timesteps\": 50000, \n",
    "         \"headless\": True,\n",
    "         \"disable_progressbar\": False}\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64effb23",
   "metadata": {},
   "source": [
    "EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_mappo_vs_baseline(mappo_agent, env, n_episodes=20):\n",
    "    results = []\n",
    "    device = mappo_agent.device\n",
    "    \n",
    "    # Set model to evaluation mode (turns off noise/exploration)\n",
    "    for model_dict in mappo_agent.models.values():\n",
    "        model_dict[\"policy\"].eval()\n",
    "    \n",
    "    print(f\"Starting MAPPO Evaluation over {n_episodes} episodes...\")\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        # 1. Reset Environment\n",
    "        obs_dict, _ = env.reset()\n",
    "        \n",
    "        # 2. Greedy Baseline (All Yaw = 0)\n",
    "        # Assuming your env wind_state is set during reset\n",
    "        env.fmodel.set(yaw_angles=np.zeros((1, 4)))\n",
    "        env.fmodel.run()\n",
    "        base_power = np.sum(env.fmodel.get_turbine_powers()) / 1e3\n",
    "        \n",
    "        # 3. MAPPO Run\n",
    "        # Convert numpy observations to torch tensors for skrl\n",
    "        with torch.no_grad():\n",
    "            torch_obs = {\n",
    "                a: torch.as_tensor(obs, device=device, dtype=torch.float32).view(1, -1) \n",
    "                for a, obs in obs_dict.items()\n",
    "            }\n",
    "            \n",
    "            # skrl MAPPO act() expects a dict of tensors\n",
    "            # We use 'act' to get the actions based on current policy\n",
    "            actions, _, _ = mappo_agent.act(torch_obs, timestep=0, timesteps=0)\n",
    "            \n",
    "            # Extract values from tensors back to numpy for FLORIS\n",
    "            yaws = np.array([actions[f\"turbine_{j}\"].cpu().numpy().flatten() for j in range(4)]).flatten() * 25.0\n",
    "        \n",
    "        # 4. Run FLORIS with MAPPO yaws\n",
    "        env.fmodel.set(yaw_angles=np.array([yaws]))\n",
    "        env.fmodel.run()\n",
    "        mappo_power = np.sum(env.fmodel.get_turbine_powers()) / 1e3\n",
    "        \n",
    "        results.append({\n",
    "            \"Episode\": i,\n",
    "            \"Base_kW\": base_power,\n",
    "            \"MAPPO_kW\": mappo_power,\n",
    "            \"Gain_%\": 100 * (mappo_power - base_power) / base_power\n",
    "        })\n",
    "        \n",
    "    print(\"Evaluation Complete.\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def plot_mappo_performance(df_results):\n",
    "    # Set the style for a clean, academic look\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # --- 1. Statistical Box Plot (The Gain Distribution) ---\n",
    "    sns.boxplot(y=df_results[\"Gain_%\"], ax=axes[0], color=\"#5da5da\", width=0.4)\n",
    "    sns.stripplot(y=df_results[\"Gain_%\"], ax=axes[0], color=\"black\", alpha=0.3)\n",
    "    \n",
    "    axes[0].axhline(0, color='red', linestyle='--', linewidth=1.2)\n",
    "    axes[0].set_title(\"Distribution of Power Gains (%)\", fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel(\"Gain over Greedy Baseline (%)\")\n",
    "\n",
    "    # --- 2. Episode Comparison (Baseline vs MAPPO) ---\n",
    "    axes[1].plot(df_results[\"Episode\"], df_results[\"Base_kW\"], label=\"Baseline (0° Yaw)\", \n",
    "                 marker='o', linestyle='-', color='gray', alpha=0.6)\n",
    "    axes[1].plot(df_results[\"Episode\"], df_results[\"MAPPO_kW\"], label=\"MAPPO Strategy\", \n",
    "                 marker='s', linestyle='-', color='#ee6677')\n",
    "    \n",
    "    axes[1].set_title(\"Power Output Comparison\", fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel(\"Episode\")\n",
    "    axes[1].set_ylabel(\"Total Farm Power (kW)\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- EXECUTE ---\n",
    "df_mappo_results = evaluate_mappo_vs_baseline(agent, env, n_episodes=20)\n",
    "print(df_mappo_results[\"Gain_%\"].describe())\n",
    "plot_mappo_performance(df_mappo_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18653493",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTUNA\n",
    "\n",
    "import optuna\n",
    "from skrl.memories.torch import RandomMemory\n",
    "from skrl.multi_agents.torch.mappo import MAPPO, MAPPO_DEFAULT_CONFIG\n",
    "from skrl.envs.wrappers.torch import wrap_env\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Hyperparameters to tune\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True)\n",
    "    entropy_scale = trial.suggest_float(\"entropy_scale\", 0.001, 0.05)\n",
    "    mini_batches = trial.suggest_categorical(\"mini_batches\", [2, 4, 8])\n",
    "\n",
    "    # 2. Env setup\n",
    "    env = FlorisMultiAgentEnv(\"data_generation/farm_types/gch.yaml\") # Ensure path is correct\n",
    "    env = wrap_env(env, wrapper=\"pettingzoo\")\n",
    "    \n",
    "    # 3. Memory & Models (Shared weights for all turbines)\n",
    "    memory = RandomMemory(memory_size=1000, num_envs=env.num_envs, device=env.device)\n",
    "    \n",
    "    # We define the Actor and Critic classes as discussed previously\n",
    "    models = {}\n",
    "    for agent_name in env.possible_agents:\n",
    "        models[agent_name] = {\n",
    "            \"policy\": Actor(env.observation_spaces[agent_name], env.action_spaces[agent_name], env.device),\n",
    "            \"value\": Critic(env.shared_observation_spaces[agent_name], env.action_spaces[agent_name], env.device)\n",
    "        }\n",
    "\n",
    "    # 4. Agent Config\n",
    "    cfg = MAPPO_DEFAULT_CONFIG.copy()\n",
    "    cfg[\"learning_rate\"] = lr\n",
    "    cfg[\"entropy_loss_scale\"] = entropy_scale\n",
    "    cfg[\"mini_batches\"] = mini_batches\n",
    "    cfg[\"experiment\"][\"write_interval\"] = 0 # Faster tuning without logs\n",
    "\n",
    "    agent = MAPPO(possible_agents=env.possible_agents, models=models, memory=memory, \n",
    "                  cfg=cfg, observation_spaces=env.observation_spaces, \n",
    "                  action_spaces=env.action_spaces, device=env.device,\n",
    "                  shared_observation_spaces=env.shared_observation_spaces)\n",
    "\n",
    "    # 5. Train and Return Result\n",
    "    agent.train(timesteps=5000) # Short training for tuning\n",
    "    # Evaluate for 1 episode\n",
    "    total_reward = 0\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(100):\n",
    "        with torch.no_grad():\n",
    "            actions = agent.act(obs, timestep=0, timesteps=0)[0]\n",
    "            obs, rewards, terminated, truncated, _ = env.step(actions)\n",
    "            total_reward += sum(rewards.values()) / len(rewards)\n",
    "            if any(terminated.values()): break\n",
    "            \n",
    "    return total_reward\n",
    "\n",
    "# Run study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"Best Params:\", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floris312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
