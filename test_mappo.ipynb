{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPO_DEFAULT_CONFIG = {\n",
    "    \"rollouts\": 16,                 # number of rollouts before updating\n",
    "    \"learning_epochs\": 8,           # number of learning epochs during each update\n",
    "    \"mini_batches\": 2,              # number of mini batches during each learning epoch\n",
    "\n",
    "    \"discount_factor\": 0.99,        # discount factor (gamma)\n",
    "    \"lambda\": 0.95,                 # TD(lambda) coefficient (lam) for computing returns and advantages\n",
    "\n",
    "    \"learning_rate\": 1e-3,                  # learning rate\n",
    "    \"learning_rate_scheduler\": None,        # learning rate scheduler class (see torch.optim.lr_scheduler)\n",
    "    \"learning_rate_scheduler_kwargs\": {},   # learning rate scheduler's kwargs (e.g. {\"step_size\": 1e-3})\n",
    "\n",
    "    \"state_preprocessor\": None,             # state preprocessor class (see skrl.resources.preprocessors)\n",
    "    \"state_preprocessor_kwargs\": {},        # state preprocessor's kwargs (e.g. {\"size\": env.observation_space})\n",
    "    \"shared_state_preprocessor\": None,      # shared state preprocessor class (see skrl.resources.preprocessors)\n",
    "    \"shared_state_preprocessor_kwargs\": {}, # shared state preprocessor's kwargs (e.g. {\"size\": env.shared_observation_space})\n",
    "    \"value_preprocessor\": None,             # value preprocessor class (see skrl.resources.preprocessors)\n",
    "    \"value_preprocessor_kwargs\": {},        # value preprocessor's kwargs (e.g. {\"size\": 1})\n",
    "\n",
    "    \"random_timesteps\": 0,          # random exploration steps\n",
    "    \"learning_starts\": 0,           # learning starts after this many steps\n",
    "\n",
    "    \"grad_norm_clip\": 0.5,              # clipping coefficient for the norm of the gradients\n",
    "    \"ratio_clip\": 0.2,                  # clipping coefficient for computing the clipped surrogate objective\n",
    "    \"value_clip\": 0.2,                  # clipping coefficient for computing the value loss (if clip_predicted_values is True)\n",
    "    \"clip_predicted_values\": False,     # clip predicted values during value loss computation\n",
    "\n",
    "    \"entropy_loss_scale\": 0.0,      # entropy loss scaling factor\n",
    "    \"value_loss_scale\": 1.0,        # value loss scaling factor\n",
    "\n",
    "    \"kl_threshold\": 0,              # KL divergence threshold for early stopping\n",
    "\n",
    "    \"rewards_shaper\": None,         # rewards shaping function: Callable(reward, timestep, timesteps) -> reward\n",
    "    \"time_limit_bootstrap\": False,  # bootstrap at timeout termination (episode truncation)\n",
    "\n",
    "    \"mixed_precision\": False,       # enable automatic mixed precision for higher performance\n",
    "\n",
    "    \"experiment\": {\n",
    "        \"directory\": \"\",            # experiment's parent directory\n",
    "        \"experiment_name\": \"\",      # experiment name\n",
    "        \"write_interval\": \"auto\",   # TensorBoard writing interval (timesteps)\n",
    "\n",
    "        \"checkpoint_interval\": \"auto\",      # interval for checkpoints (timesteps)\n",
    "        \"store_separately\": False,          # whether to store checkpoints separately\n",
    "\n",
    "        \"wandb\": False,             # whether to use Weights & Biases\n",
    "        \"wandb_kwargs\": {}          # wandb kwargs (see https://docs.wandb.ai/ref/python/init)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cba02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrl.multi_agents.torch.mappo import MAPPO, MAPPO_DEFAULT_CONFIG\n",
    "\n",
    "# import the environment wrapper\n",
    "from skrl.envs.wrappers.torch import wrap_env\n",
    "\n",
    "# import a PettingZoo environment\n",
    "from pettingzoo.sisl import multiwalker_v9\n",
    "\n",
    "# load the environment\n",
    "env = multiwalker_v9.parallel_env()\n",
    "\n",
    "# wrap the environment\n",
    "env = wrap_env(env)  # or 'env = wrap_env(env, wrapper=\"pettingzoo\")'\n",
    "\n",
    "# import the memory class\n",
    "from skrl.memories.torch import RandomMemory\n",
    "\n",
    "# instantiate the memory (assumes there is a wrapped environment: env)\n",
    "memory = RandomMemory(memory_size=1000, num_envs=env.num_envs, device=env.device)\n",
    "\n",
    "# instantiate the agent's models\n",
    "models = {}\n",
    "for agent_name in env.possible_agents:\n",
    "    models[agent_name] = {}\n",
    "    models[agent_name][\"policy\"] = ...\n",
    "    models[agent_name][\"value\"] = ...  # only required during training\n",
    "\n",
    "# adjust some configuration if necessary\n",
    "cfg_agent = MAPPO_DEFAULT_CONFIG.copy()\n",
    "cfg_agent[\"<KEY>\"] = ...\n",
    "\n",
    "# instantiate the agent\n",
    "# (assuming a defined environment <env> and memories <memories>)\n",
    "agent = MAPPO(possible_agents=env.possible_agents,\n",
    "              models=models,\n",
    "              memory=memories,  # only required during training\n",
    "              cfg=cfg_agent,\n",
    "              observation_spaces=env.observation_spaces,\n",
    "              action_spaces=env.action_spaces,\n",
    "              device=env.device,\n",
    "              shared_observation_spaces=env.shared_observation_spaces)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
